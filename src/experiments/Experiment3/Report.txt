**Experiment Report: Run 3**

---

### 1. Experiment Setup
**Objective:** Evaluate the impact of fine-tuning hyperparameters on accuracy and generalization.

**Changes from Run 2:**
- **Learning Rate:** Reduced from 0.0005 to 0.0003 to stabilize training.
- **Batch Size:** Increased from 64 to 128 for better generalization.
- **Regularization:** Increased dropout from 0.3 to 0.4 to reduce overfitting.
- **Weight Decay:** Increased from 1e-4 to 5e-4 to encourage smaller weights.

### 2. Training and Validation Metrics

**Epoch-wise Performance:**
```
Epoch [1/20], Train Loss: 0.8312, Val Loss: 2.7220, Val Acc: 17.20%
Epoch [2/20], Train Loss: 0.2409, Val Loss: 0.4002, Val Acc: 80.93%
Epoch [3/20], Train Loss: 0.2059, Val Loss: 0.1730, Val Acc: 92.53%
Epoch [4/20], Train Loss: 0.1924, Val Loss: 0.1834, Val Acc: 92.40%
Epoch [5/20], Train Loss: 0.1650, Val Loss: 0.1589, Val Acc: 93.33%
Epoch [6/20], Train Loss: 0.1378, Val Loss: 0.1658, Val Acc: 92.93%
Epoch [7/20], Train Loss: 0.1145, Val Loss: 0.2187, Val Acc: 90.80%
Epoch [8/20], Train Loss: 0.1073, Val Loss: 0.1660, Val Acc: 92.80%
Epoch [9/20], Train Loss: 0.1184, Val Loss: 0.2060, Val Acc: 92.40%
Epoch [10/20], Train Loss: 0.0797, Val Loss: 0.2456, Val Acc: 91.07%
Early stopping triggered.
```

**Test Performance:**
- **Test Loss:** 0.2685
- **Test Accuracy:** 89.73%

### 3. Classification Report
```
              precision    recall  f1-score   support

         Few       0.85      0.98      0.91       122
      Medium       0.84      0.89      0.87       277
        Many       0.97      0.87      0.92       351

    accuracy                           0.90       750
   macro avg       0.89      0.92      0.90       750
weighted avg       0.90      0.90      0.90       750
```

### 4. Observations
- **Early Stopping:** Triggered at epoch 10, suggesting diminishing returns beyond this point.
- **Validation Accuracy:** Peaked at **93.33%**, showing improvement over previous runs.
- **Confusion Matrix Trends:**
  - Fewer misclassifications in the "Few" and "Many" classes.
  - "Medium" still shows some confusion with "Many," but slightly better than in previous runs.
- **Overfitting Risk:** The increasing validation loss towards the end may indicate overfitting.

### 5. Next Steps
- Tune dropout rate further to balance generalization.
- Experiment with reducing weight decay slightly.
- Try different learning rate schedules (e.g., cosine annealing).
- Investigate alternative optimizers like **RMSprop or SGD with momentum**.

---

**End of Report**

